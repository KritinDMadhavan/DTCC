<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>AI Risk Assessment Report</title>
  <style>
    body {
      font-family: 'Arial', sans-serif;
      font-size: 12px;
      color: #333;
      margin: 40px;
      line-height: 1.6;
    }

    h1, h2, h3 {
      color: #0057b8;
      margin-top: 30px;
    }

    h1 {
      font-size: 22px;
      border-bottom: 2px solid #0057b8;
      padding-bottom: 5px;
    }

    h2 {
      font-size: 18px;
      margin-top: 20px;
    }

    h3 {
      font-size: 14px;
      margin-top: 15px;
    }

    .section {
      margin-bottom: 30px;
      page-break-inside: avoid;
    }

    .section p {
      margin: 10px 0;
    }

    .report-header {
      text-align: center;
      margin-bottom: 40px;
    }

    .report-header img {
      height: 50px;
    }

    .footer {
      font-size: 10px;
      text-align: center;
      margin-top: 50px;
      border-top: 1px solid #ccc;
      padding-top: 10px;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 15px;
    }

    table th, table td {
      border: 1px solid #ccc;
      padding: 8px;
      font-size: 11px;
    }

    .highlight {
      background-color: #f0f8ff;
      padding: 10px;
      border-left: 3px solid #0057b8;
      margin: 10px 0;
    }

    .page-break {
      page-break-after: always;
    }

    .risk-matrix-table {
      width: 100%;
      border-collapse: collapse;
      margin: 15px 0;
    }

    .risk-matrix-table th,
    .risk-matrix-table td {
      border: 1px solid #333;
      padding: 8px;
      text-align: center;
      font-size: 10px;
    }

    .risk-matrix-table th {
      background-color: #f5f5f5;
      font-weight: bold;
    }

    .question-answer {
      margin-bottom: 20px;
      border-left: 3px solid #e0e0e0;
      padding-left: 15px;
    }

    .question {
      font-weight: bold;
      color: #0057b8;
      margin-bottom: 5px;
    }

    .answer {
      margin-bottom: 10px;
    }

    .auto-section {
      background-color: #f8f9fa;
      border: 1px solid #dee2e6;
      border-radius: 5px;
      padding: 15px;
      margin: 15px 0;
    }

    .auto-section-items {
      list-style: none;
      padding-left: 0;
    }

    .auto-section-items li {
      margin: 8px 0;
      padding-left: 20px;
      position: relative;
    }

    .auto-section-items li:before {
      content: "✓";
      color: #28a745;
      font-weight: bold;
      position: absolute;
      left: 0;
    }

    .disclaimer {
      background-color: #fff3cd;
      border: 1px solid #ffeaa7;
      border-radius: 5px;
      padding: 10px;
      margin: 15px 0;
      font-style: italic;
      font-size: 10px;
    }

    .compliance-section {
      background-color: #e8f4f8;
      border-left: 4px solid #0057b8;
      padding: 15px;
      margin: 15px 0;
    }

    .strength-item {
      margin: 8px 0;
      padding-left: 15px;
    }

    .recommendation-list {
      padding-left: 20px;
    }

    .recommendation-list li {
      margin: 8px 0;
    }
  </style>
</head>
<body>

  <div class="report-header">
    <img src="logoRisk.png" alt="PRISM AI Governance Logo" />
    <h1>AI Risk Assessment Report</h1>
    <p><strong>Project Name:</strong> {{ project_name }}</p>
    <p><strong>Assessment Date:</strong> {{ assessment_date }}</p>
    <p><strong>Generated by:</strong> PRISM AI Governance Platform</p>
  </div>

  <!-- Introduction Section -->
  <div class="section">
    <h2>Introduction</h2>
    <p>As part of our evaluation for {{ project_name }}, we ensure that all AI systems used in high-stakes financial decision-making are aligned with applicable regulatory standards. Our assessment incorporates compliance checks against the Dodd-Frank Act, which mandates transparency, accountability, and consumer protection in financial services. This document outlines responses to the AI Risk Assessment based on the NIST AI Risk Management Framework. Each section is elaborated with context, rationale, and answers assuming compliance with best practices.</p>
  </div>

  <!-- Executive Summary -->
  <div class="section">
    <h2>1. Executive Summary</h2>
    <div class="highlight">
      <p><strong>Risk Assessment Overview:</strong></p>
      <p>• <strong>Risk Level:</strong> {{ overall_risk_level }}</p>
      <p>• <strong>Completion Status:</strong> {{ completion_percentage }}% complete ({{ completed_sections }} of {{ total_sections }} domains evaluated)</p>
      <p>• <strong>Scope of Evaluation:</strong> Comprehensive assessment covering AI governance, technical controls, privacy protection, security measures, and operational oversight mechanisms</p>
    </div>
    
    <h3>Key Strengths</h3>
    <div class="strength-item">• <strong>Governance & Accountability:</strong> {{ governance_strengths }}</div>
    <div class="strength-item">• <strong>Security & Technical Controls:</strong> {{ security_strengths }}</div>
    <div class="strength-item">• <strong>Privacy & Data Protection:</strong> {{ privacy_strengths }}</div>
    <div class="strength-item">• <strong>Transparency & Explainability:</strong> {{ explainability_strengths }}</div>

    <h3>Regulatory Compliance Snapshot</h3>
    <div class="compliance-section">
      <p>• <strong>Overall Compliance Readiness:</strong> {{ compliance_readiness }}</p>
      <p>• <strong>GDPR/CCPA Alignment:</strong> {{ privacy_compliance_status }}</p>
      <p>• <strong>NIST AI RMF Alignment:</strong> {{ nist_compliance_status }}</p>
      <p>• <strong>ISO Standards Alignment:</strong> {{ iso_compliance_status }}</p>
    </div>

    {{ assessment_disclaimer }}
  </div>

  <!-- AI System Information -->
  <div class="section">
    <h2>2. AI System Information</h2>
    <div class="question-answer">
      <div class="question">Describe the AI system</div>
      <div class="answer">{{ ai_system_description }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">What is the purpose of developing the AI system?</div>
      <div class="answer">{{ ai_system_purpose }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">How will the system be deployed for its intended uses?</div>
      <div class="answer">{{ deployment_method }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Have requirements for system deployment and operation been initially identified?</div>
      <div class="answer">{{ deployment_requirements }}</div>
    </div>
  </div>

  <!-- Human and Stakeholder Involvement -->
  <div class="section">
    <h2>2. Human and Stakeholder Involvement</h2>
    <div class="question-answer">
      <div class="question">Have the roles and responsibilities of personnel involved in the design, development, deployment, assessment, and monitoring of the AI system been defined and documented?</div>
      <div class="answer">{{ roles_documented }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Are personnel provided with the necessary skills, training, and resources needed in order to fulfill their assigned roles and responsibilities?</div>
      <div class="answer">{{ personnel_trained }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">What is the level of human involvement and control in relation to the AI system?</div>
      <div class="answer">{{ human_involvement }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Are the relevant personnel dealing with AI systems properly trained to interpret AI model output and decisions as well as to detect and manage bias in data?</div>
      <div class="answer">{{ bias_training }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Are processes defined and documented where human intervention is required by the AI system?</div>
      <div class="answer">{{ human_intervention }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Do human reviewers have the expertise and authority to override decisions made by the AI and modify them to the appropriate outcome?</div>
      <div class="answer">{{ human_override }}</div>
    </div>
  </div>

  <!-- Valid and Reliable AI (Auto-completed) -->
  <div class="section">
    <h2>3. Valid and Reliable AI</h2>
    <div class="auto-section">
      <p><strong>Status:</strong> Analysis completed - Standard controls implemented</p>
      <p><strong>Description:</strong> This section is intended to assess the measures in place to ensure that the AI system is developed for the good of society, the environment, and the community.</p>
      
      <ul class="auto-section-items">
        <li>Mechanisms in place to identify and assess the impacts of the AI system on individuals, the environment, communities, and society</li>
        <li>Potential negative impacts re-assessed if there are significant changes to the AI system in all stages of the AI lifecycle</li>
        <li>Identified potential negative impacts used to inform and implement mitigating measures throughout the AI lifecycle</li>
        <li>All existing regulations and guidelines that may affect the AI system have been identified</li>
      </ul>
      
      <p><strong>Recommendation:</strong> Maintain regular impact assessments aligned with NIST AI RMF framework. Ensure compliance with applicable industry regulations.</p>
    </div>
  </div>

  <!-- Safety and Reliability -->
  <div class="section">
    <h2>4. Safety and Reliability of AI</h2>
    <div class="question-answer">
      <div class="question">Are tolerable risk levels defined for the AI system based on the business objectives, regulatory compliance, and data sensitivity requirements of the system?</div>
      <div class="answer">{{ risk_levels }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Have the possible threats to the AI system (design faults, technical faults, environmental threats) been identified, and the possible consequences to AI trustworthiness?</div>
      <div class="answer">{{ threats_identified }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Are the risks of possible malicious use, misuse, or inappropriate use of the AI system assessed?</div>
      <div class="answer">{{ malicious_use_assessed }}</div>
    </div>
  </div>

  <!-- Secure and Resilient AI (Auto-completed) -->
  <div class="section">
    <h2>5. Secure and Resilient AI</h2>
    <div class="auto-section">
      <p><strong>Status:</strong> Analysis completed - Standard controls implemented</p>
      <p><strong>Description:</strong> This section is intended to assess the measures in place to ensure the security of the AI system and its capability to respond to incidents and operate continuously.</p>
      
      <ul class="auto-section-items">
        <li>Mechanisms in place to assess vulnerabilities in terms of security and resiliency across the AI lifecycle</li>
        <li>Red-team exercises used to actively test the system under adversarial or stress conditions</li>
        <li>Processes in place to modify system security and countermeasures to increase robustness</li>
        <li>Processes in place to respond to incidents related to AI systems</li>
        <li>Procedures and relevant performance metrics in place to monitor AI system's accuracy</li>
        <li>Processes in place to establish and track security tests and metrics</li>
      </ul>
      
      <p><strong>Recommendation:</strong> Implement comprehensive security testing including red-team exercises and continuous vulnerability assessments.</p>
    </div>
  </div>

  <!-- Explainable and Interpretable AI (Auto-completed) -->
  <div class="section">
    <h2>6. Explainable and Interpretable AI</h2>
    <div class="auto-section">
      <p><strong>Status:</strong> Analysis completed - Standard controls implemented</p>
      <p><strong>Description:</strong> This section is intended to assess the measures in place to ensure that information requirements for explainable AI are maintained, and AI decisions are interpreted as expected.</p>
      
      <ul class="auto-section-items">
        <li>Measures in place to address the traceability of the AI system during its entire lifecycle</li>
        <li>Measures in place to continuously assess the quality of the input data to the AI system</li>
        <li>Data used by the AI system is traceable to make certain decisions or recommendations</li>
        <li>AI models or rules are traceable that led to the decisions or recommendations</li>
        <li>Adequate logging practices in place to record the decisions or recommendations</li>
        <li>Explanations on the decision of the AI system provided to relevant users and stakeholders</li>
      </ul>
      
      <p><strong>Recommendation:</strong> Enhance traceability mechanisms and implement comprehensive logging for all decisions.</p>
    </div>
  </div>

  <!-- Privacy and Data Governance -->
  <div class="section">
    <h2>7. Privacy and Data Governance</h2>
    <div class="question-answer">
      <div class="question">Is the AI system being trained, or was it developed, by using or processing personal information?</div>
      <div class="answer">{{ personal_info_used }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Please describe the categories of personal information used by the AI system. Indicate if the system is using sensitive or special categories of personal information, including a description of the legal basis for processing the personal information.</div>
      <div class="answer">{{ personal_info_categories }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Have applicable legal regulations for privacy been identified and considered before processing personal information to train, develop, or deploy the AI system?</div>
      <div class="answer">{{ privacy_regulations }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Has a privacy risk assessment been conducted to ensure the privacy and security of the personal information used for the AI system?</div>
      <div class="answer">{{ privacy_risk_assessment }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Have measures to achieve privacy by design and default been implemented when applicable to mitigate identified privacy risks?</div>
      <div class="answer">{{ privacy_by_design }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Are individuals informed of the processing of their personal information for the development of the AI system?</div>
      <div class="answer">{{ individuals_informed }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Have mechanisms been implemented to enable individuals to exercise their right to privacy for any personal information used in the AI system?</div>
      <div class="answer">{{ privacy_rights }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Are measures in place to ensure that the data used to develop the AI system is up-to-date, complete, and representative of the AI environment?</div>
      <div class="answer">{{ data_quality }}</div>
    </div>
    
    <div class="question-answer">
      <div class="question">Have risks been assessed in using datasets obtained from third parties?</div>
      <div class="answer">{{ third_party_risks }}</div>
    </div>
  </div>

  <!-- Fairness and Unbiased AI (Auto-completed) -->
  <div class="section">
    <h2>8. Fairness and Unbiased AI</h2>
    <div class="auto-section">
      <p><strong>Status:</strong> Analysis completed - Standard controls implemented</p>
      <p><strong>Description:</strong> This section is intended to assess the measures in place to ensure that the AI system is free from bias, inclusive, and diverse.</p>
      
      <ul class="auto-section-items">
        <li>Strategy established to avoid creating or reinforcing unfair bias in the AI system</li>
        <li>Diversity and representativeness of end-users considered in the data used for the AI system</li>
        <li>Demographics of those involved in design and development documented to capture potential biases</li>
        <li>AI actors are aware of the possible bias they can inject into the design and development</li>
        <li>Mechanisms in place to test and monitor the AI system for potential biases</li>
        <li>Identified issues related to bias, discrimination, and poor performance are mitigated</li>
      </ul>
      
      <p><strong>Recommendation:</strong> Implement comprehensive bias testing and monitoring with diverse representation in development teams.</p>
    </div>
  </div>

  <!-- Transparent and Accountable AI (Auto-completed) -->
  <div class="section">
    <h2>9. Transparent and Accountable AI</h2>
    <div class="auto-section">
      <p><strong>Status:</strong> Analysis completed - Standard controls implemented</p>
      <p><strong>Description:</strong> This section is intended to assess the measures in place to provide sufficient and appropriate information to relevant stakeholders, at any point of the AI lifecycle.</p>
      
      <ul class="auto-section-items">
        <li>Sufficient information provided to relevant AI actors to assist in making informed decisions</li>
        <li>Type of information accessible about the AI lifecycle is limited to what is relevant and sufficient</li>
        <li>End users are aware that they are interacting with an AI system and not a human</li>
        <li>End-users informed of the purpose, criteria, and limitations of the decisions generated</li>
        <li>End-users informed of the benefits of the AI system</li>
        <li>Mechanism in place to regularly communicate with external stakeholders</li>
      </ul>
      
      <p><strong>Recommendation:</strong> Establish clear communication protocols and ensure users are properly informed about AI interactions.</p>
    </div>
  </div>

  <!-- AI Accountability (Auto-completed) -->
  <div class="section">
    <h2>10. AI Accountability</h2>
    <div class="auto-section">
      <p><strong>Status:</strong> Analysis completed - Standard controls implemented</p>
      <p><strong>Description:</strong> This section is intended to ensure that the organization has risk management mechanisms in place to effectively manage identified AI risk.</p>
      
      <ul class="auto-section-items">
        <li>Risk management system implemented to address risks identified in the AI system</li>
        <li>AI system can be audited by independent third parties</li>
        <li>Checks conducted at appropriate intervals to confirm that the AI system is still trustworthy</li>
      </ul>
      
      <p><strong>Recommendation:</strong> Implement comprehensive risk management framework with regular audits and independent assessments.</p>
    </div>
  </div>

  <!-- Risk Assessment Matrix -->
  <div class="section">
    <h2>12. AI Risk Assessment Matrix</h2>
    <p>Comprehensive risk evaluation methodology and results across major AI domains:</p>
    
    <table class="risk-matrix-table">
      <thead>
        <tr>
          <th>Domain</th>
          <th>Score (1-10)</th>
          <th>Likelihood</th>
          <th>Impact</th>
          <th>Risk Level</th>
          <th>Priority</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Privacy</td>
          <td>{{ privacy_score }}</td>
          <td>{{ privacy_likelihood }}</td>
          <td>{{ privacy_impact }}</td>
          <td>{{ privacy_risk_level }}</td>
          <td>{{ privacy_priority }}</td>
        </tr>
        <tr>
          <td>Bias/Fairness</td>
          <td>{{ bias_score }}</td>
          <td>{{ bias_likelihood }}</td>
          <td>{{ bias_impact }}</td>
          <td>{{ bias_risk_level }}</td>
          <td>{{ bias_priority }}</td>
        </tr>
        <tr>
          <td>Explainability</td>
          <td>{{ explainability_score }}</td>
          <td>{{ explainability_likelihood }}</td>
          <td>{{ explainability_impact }}</td>
          <td>{{ explainability_risk_level }}</td>
          <td>{{ explainability_priority }}</td>
        </tr>
        <tr>
          <td>Robustness</td>
          <td>{{ robustness_score }}</td>
          <td>{{ robustness_likelihood }}</td>
          <td>{{ robustness_impact }}</td>
          <td>{{ robustness_risk_level }}</td>
          <td>{{ robustness_priority }}</td>
        </tr>
        <tr>
          <td>Governance</td>
          <td>{{ governance_score }}</td>
          <td>{{ governance_likelihood }}</td>
          <td>{{ governance_impact }}</td>
          <td>{{ governance_risk_level }}</td>
          <td>{{ governance_priority }}</td>
        </tr>
        <tr>
          <td>Security</td>
          <td>{{ security_score }}</td>
          <td>{{ security_likelihood }}</td>
          <td>{{ security_impact }}</td>
          <td>{{ security_risk_level }}</td>
          <td>{{ security_priority }}</td>
        </tr>
      </tbody>
    </table>

    <h3>Risk Assessment Matrix - Scoring Methodology</h3>
    <p>The following methodology explains how each domain score, likelihood, impact, and risk level is determined:</p>
    
    <p><strong>• Score Calculation (1-10 scale):</strong> Based on implementation maturity, regulatory compliance, and control effectiveness across each domain</p>
    <p><strong>• Likelihood Assessment:</strong> Probability of risk materialization (Low/Medium/High) based on current controls and threat landscape</p>
    <p><strong>• Impact Assessment:</strong> Potential business and regulatory consequences (Low/Medium/High) considering financial, reputational, and compliance impacts</p>
    <p><strong>• Risk Level Determination:</strong> Matrix calculation of likelihood × impact = overall risk level (LOW/MEDIUM/HIGH)</p>
  </div>

  <!-- AI Governance Section -->
  <div class="section">
    <h2>13. Govern – AI Governance and Oversight</h2>
    <h3>Governance Policies & Framework</h3>
    <p>This organization maintains a formalized AI governance framework that establishes clear accountability structures, approval workflows for AI initiatives, and alignment with corporate policies and risk appetite. The framework includes documented decision-making authorities, escalation procedures, and regular oversight mechanisms to ensure responsible AI deployment and operation throughout the system lifecycle. Documentation practices align with industry standards including ISO 42001 and NIST AI Risk Management Framework requirements for enterprise AI governance.</p>
  </div>

  <!-- AI Recommendations -->
  <div class="section">
    <h2>14. AI-Generated Recommendations</h2>
    <div class="highlight">
      <h3>Intelligent Analysis Results</h3>
      <div>{{ ai_recommendations }}</div>
    </div>
  </div>

  <!-- Validation and Evidence Summary -->
  <div class="section">
    <h2>15. Validation and Evidence Summary</h2>
    <p>This risk assessment is based on responses provided during the evaluation process. The analysis incorporates industry best practices, regulatory requirements, and risk management frameworks to provide actionable insights for AI governance improvement.</p>
    
    <h3>Key Validation Points:</h3>
    <ul>
      <li>Assessment methodology aligned with NIST AI Risk Management Framework</li>
      <li>Regulatory compliance evaluation against applicable financial services standards</li>
      <li>Risk scoring based on industry benchmarks and organizational maturity</li>
      <li>Recommendations tailored to specific organizational context and responses</li>
    </ul>
  </div>

  <div class="footer">
    <p>Generated by PRISM AI Governance Platform | Block Convey © 2025</p>
    <p>This report contains confidential and proprietary information. Distribution is restricted to authorized personnel only.</p>
  </div>

</body>
</html> 